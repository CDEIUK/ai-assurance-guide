---
title: Challenges for AI assurance
---

import CookieBanner from "../components/cookies"
import Collapse from "../components/collapse"
import OutboundLink from "../components/outbound-link"
import License from "../components/license"

<CookieBanner />

## A mature ecosystem will require ongoing effort

We have deliberately used the word ecosystem to describe the network of roles needed for assurance. These are interdependent roles that co-evolve, and rely on a balance between competing interests between different actors.

There are some fundamental tensions of assurance ecosystems that can be managed, but do not go away even in mature ecosystems, such as gaming, conflicts of interest, the power dynamics behind technical standards making, balancing innovation and risk minimisation, and responsibility for good AI decisions.

For example, a key tension arises due to different trade-offs between risk minimisation and encouraging innovation. Governments, regulators, developers, and executives face the risks and benefits of these technologies differently, and often have incentives that are in tension. To resolve this, a balanced approach is needed to consider the risks and benefits across the ecosystem. The ideal arrangement is one where assurance acts to give greater confidence to both developers and purchasers of AI systems and enables wider, safer adoption.

A further tension arises between executives and developers around accepting responsibility for good AI decisions. Developers and executives may have the same goal - for example to have fair and safe outcomes - but each would like the other party to ultimately be accountable. Developers may say that they are creating a tool and it’s up to executives to use it correctly. Executives may say they are using a tool and it’s up to developers to make it work well. In practice, both may be responsible, but regulators have not yet provided the guidance necessary to resolve this tension. This tension is particularly pertinent in the AI context where executives often procure AI technology rather than building tools in-house.

TODO: add image

Because of these fundamental tensions, even mature ecosystems can fail, as we have seen in places like Dieselgate (gaming) and financial crises (AAA rated MBS ignoring correlated risk), though a well functioning assurance ecosystem makes these the exception rather than the rule.

Preventing and mitigating these challenges requires vigilance and scrutiny, and cannot be solved by a ‘set-and-forget’ testing regime. Additional challenges are posed to AI assurance by the autonomous, complex and scalable nature of AI, which make it more difficult to keep track of who is accountable, where they are accountable and what they are accountable for.

Despite the presence of tensions, an assurance ecosystem needs to be built on effective coordination between different roles. An ecosystem that relies too heavily on the role of one player might work for a while, but will be unstable due to the different incentives of the various roles. Whether that is a central regulator or reliance on the self assessed reports of developers, overreliance on one player will fail to reflect the needs and interests of actors across the ecosystem.

If only central regulators are relied on, capacity for assurance will be too low to keep up with demand, hindering innovation. Similarly, If assurance is over-reliant upon the self-assessment of developers, the ecosystem will lack the supporting structures that determine good practice and build trust and trustworthiness.

It will be important for all actors in AI Assurance to play their part in building a trusted and trustworthy AI assurance ecosystem.

<License />
