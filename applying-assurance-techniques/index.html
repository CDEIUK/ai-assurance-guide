<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><script>window.fetch = undefined;</script><script src="/ai-assurance-guide/fetch.umd.js"></script><script src="/ai-assurance-guide/pace.min.js"></script><link href="/ai-assurance-guide/pace.css" rel="stylesheet"/><style data-href="/ai-assurance-guide/styles.517ff5f83aa9c14cf3ec.css">.button{background:#00a886;border:0;border-radius:4px;box-shadow:0 4px #030;color:#fff;cursor:pointer;flex:0 0 auto;font-size:18px;font-weight:700;padding:10px 15px;margin:15px}.button:active,.button:hover{background-color:#008066}.button:active{box-shadow:0 2px #030;transform:translateY(4px)}.container{align-items:baseline;background:#aaa;color:#fff;font-weight:700;display:flex;flex-wrap:wrap;justify-content:space-between;left:0;position:fixed;width:100%;z-index:999}.content{flex:0 0 95%;margin:15px}.overlay{position:fixed;left:0;top:0;width:100%;height:100%;z-index:999;background-color:rgba(0;0;0;0.3)}input[type=radio]{margin:5px 10px 10px 0;width:20px;height:20px;vertical-align:middle;cursor:pointer}.accordion{border:1px solid rgba(0,0,0,.1);margin-top:1em;margin-bottom:1em}.accordion__item+.accordion__item{border-top:1px solid rgba(0,0,0,.1)}.accordion__button{background-color:#00a885;color:#fff;cursor:pointer;padding:18px;width:100%;text-align:left;border:none}.accordion__button:hover{background-color:rgba(0,168,133,.8)}.accordion__button:before{display:inline-block;content:"";height:10px;width:10px;margin-right:12px;border-bottom:2px solid;border-right:2px solid;transform:rotate(-45deg);transition:transform .3s ease}.accordion__button[aria-expanded=true]:before,.accordion__button[aria-selected=true]:before{transform:translate(25%,25%) rotate(45deg) translate(-25%,-25%);transition:transform .3s ease}.accordion__panel{padding:1em 2em .5em;animation:fadein .3s ease-in}.accordion__panel h3{background-color:red;padding:.5em 0 .5em 1.8em;margin-left:-1.8em;max-width:15em;color:#fff;background-color:#262445;letter-spacing:.075em;font-family:Open Sans,sans-serif}.accordion__panel h3 .anchor.before{color:#fff!important;padding:.5em 0;left:1.5em;fill:#fff}@keyframes fadein{0%{opacity:0}to{opacity:1}}code.inline-code{background-color:#00a886!important}.narrow-screen-warning{display:none}.js-plotly-plot>.plot-container>div.svg-container{margin:0 auto!important}@media screen and (max-width:600px){.js-plotly-plot{display:none}.narrow-screen-warning{display:block;padding:1em}}</style><meta name="generator" content="Gatsby 2.23.4"/><link rel="preconnect dns-prefetch" href="https://www.googletagmanager.com"/><link rel="preconnect dns-prefetch" href="https://www.google-analytics.com"/><style type="text/css">
    .anchor.before {
      position: absolute;
      top: 0;
      left: 0;
      transform: translateX(-100%);
      padding-right: 4px;
    }
    .anchor.after {
      display: inline-block;
      padding-left: 4px;
    }
    h1 .anchor svg,
    h2 .anchor svg,
    h3 .anchor svg,
    h4 .anchor svg,
    h5 .anchor svg,
    h6 .anchor svg {
      visibility: hidden;
    }
    h1:hover .anchor svg,
    h2:hover .anchor svg,
    h3:hover .anchor svg,
    h4:hover .anchor svg,
    h5:hover .anchor svg,
    h6:hover .anchor svg,
    h1 .anchor:focus svg,
    h2 .anchor:focus svg,
    h3 .anchor:focus svg,
    h4 .anchor:focus svg,
    h5 .anchor:focus svg,
    h6 .anchor:focus svg {
      visibility: visible;
    }
  </style><script>
    document.addEventListener("DOMContentLoaded", function(event) {
      var hash = window.decodeURI(location.hash.replace('#', ''))
      if (hash !== '') {
        var element = document.getElementById(hash)
        if (element) {
          var scrollTop = window.pageYOffset || document.documentElement.scrollTop || document.body.scrollTop
          var clientTop = document.documentElement.clientTop || document.body.clientTop || 0
          var offset = element.getBoundingClientRect().top + scrollTop - clientTop
          // Wait for the browser to finish rendering before scrolling.
          setTimeout((function() {
            window.scrollTo(0, offset - 0)
          }), 0)
        }
      }
    })
  </script><title data-react-helmet="true">Applying assurance techniques to AI | AI assurance guide</title><meta data-react-helmet="true" name="description" content="AI assurance guide"/><meta data-react-helmet="true" name="image" content="https://cdeiuk.github.io/ai-assurance-guide/banner.png"/><meta data-react-helmet="true" http-equiv="x-ua-compatible" content="IE=edge,chrome=1"/><meta data-react-helmet="true" name="MobileOptimized" content="320"/><meta data-react-helmet="true" name="HandheldFriendly" content="True"/><meta data-react-helmet="true" name="google" content="notranslate"/><meta data-react-helmet="true" name="referrer" content="no-referrer-when-downgrade"/><meta data-react-helmet="true" property="og:url" content="https://cdeiuk.github.io/ai-assurance-guide/applying-assurance-techniques/"/><meta data-react-helmet="true" property="og:type" content="website"/><meta data-react-helmet="true" property="og:title" content="Applying assurance techniques to AI | AI assurance guide"/><meta data-react-helmet="true" property="og:description" content="AI assurance guide"/><meta data-react-helmet="true" property="og:locale" content="en"/><meta data-react-helmet="true" property="og:site_name" content="AI assurance guide"/><meta data-react-helmet="true" property="og:image" content="https://cdeiuk.github.io/ai-assurance-guide/banner.png"/><meta data-react-helmet="true" property="og:image:secure_url" content="https://cdeiuk.github.io/ai-assurance-guide/banner.png"/><meta data-react-helmet="true" property="og:image:alt" content="Banner"/><meta data-react-helmet="true" property="og:image:type" content="image/png"/><meta data-react-helmet="true" property="og:image:width" content="1200"/><meta data-react-helmet="true" property="og:image:height" content="630"/><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"/><meta data-react-helmet="true" name="twitter:title" content="Applying assurance techniques to AI | AI assurance guide"/><meta data-react-helmet="true" name="twitter:site" content="@rocketseat"/><meta data-react-helmet="true" name="twitter:creator" content="@rocketseat"/><meta data-react-helmet="true" name="twitter:image" content="https://cdeiuk.github.io/ai-assurance-guide/banner.png"/><meta data-react-helmet="true" name="twitter:image:src" content="https://cdeiuk.github.io/ai-assurance-guide/banner.png"/><meta data-react-helmet="true" name="twitter:image:alt" content="Banner"/><meta data-react-helmet="true" name="twitter:image:width" content="1200"/><meta data-react-helmet="true" name="twitter:image:height" content="630"/><script data-react-helmet="true" type="application/ld+json">[{"@context":"http://schema.org","@type":"WebSite","url":"https://cdeiuk.github.io/ai-assurance-guide/applying-assurance-techniques/","name":"Applying assurance techniques to AI","alternateName":"AI assurance guide"}]</script><link rel="icon" href="/ai-assurance-guide/favicon-32x32.png?v=80ec5df783a2635a77990defac450c75"/><link rel="manifest" href="/ai-assurance-guide/manifest.webmanifest"/><link rel="apple-touch-icon" sizes="48x48" href="/ai-assurance-guide/icons/icon-48x48.png?v=80ec5df783a2635a77990defac450c75"/><link rel="apple-touch-icon" sizes="72x72" href="/ai-assurance-guide/icons/icon-72x72.png?v=80ec5df783a2635a77990defac450c75"/><link rel="apple-touch-icon" sizes="96x96" href="/ai-assurance-guide/icons/icon-96x96.png?v=80ec5df783a2635a77990defac450c75"/><link rel="apple-touch-icon" sizes="144x144" href="/ai-assurance-guide/icons/icon-144x144.png?v=80ec5df783a2635a77990defac450c75"/><link rel="apple-touch-icon" sizes="192x192" href="/ai-assurance-guide/icons/icon-192x192.png?v=80ec5df783a2635a77990defac450c75"/><link rel="apple-touch-icon" sizes="256x256" href="/ai-assurance-guide/icons/icon-256x256.png?v=80ec5df783a2635a77990defac450c75"/><link rel="apple-touch-icon" sizes="384x384" href="/ai-assurance-guide/icons/icon-384x384.png?v=80ec5df783a2635a77990defac450c75"/><link rel="apple-touch-icon" sizes="512x512" href="/ai-assurance-guide/icons/icon-512x512.png?v=80ec5df783a2635a77990defac450c75"/><link rel="sitemap" type="application/xml" href="/ai-assurance-guide/sitemap.xml"/><link rel="canonical" href="https://cdeiuk.github.io/ai-assurance-guide/applying-assurance-techniques/" data-baseprotocol="https:" data-basehost="cdeiuk.github.io"/><link as="script" rel="preload" href="/ai-assurance-guide/webpack-runtime-076fa458eaac626e98e4.js"/><link as="script" rel="preload" href="/ai-assurance-guide/framework-95b680cb3190aa9bfe59.js"/><link as="script" rel="preload" href="/ai-assurance-guide/styles-24c541b6ac347bae38f1.js"/><link as="script" rel="preload" href="/ai-assurance-guide/f0e45107-1b350d56f045fa1c0ff3.js"/><link as="script" rel="preload" href="/ai-assurance-guide/app-39b77a34e7ba1018ce1d.js"/><link as="script" rel="preload" href="/ai-assurance-guide/545f34e4-5ad47dd60d402bdffae8.js"/><link as="script" rel="preload" href="/ai-assurance-guide/5e2a4920-b71d4aaefb263d56b92c.js"/><link as="script" rel="preload" href="/ai-assurance-guide/252f366e-e522de43b84fd218bea8.js"/><link as="script" rel="preload" href="/ai-assurance-guide/a7629950cb7a18c7593baf47d0acfe798f1cddc4-bc84a563ac4ce2a5b404.js"/><link as="script" rel="preload" href="/ai-assurance-guide/component---node-modules-rocketseat-gatsby-theme-docs-core-src-templates-docs-query-js-61d59f35b4662c9277ef.js"/><link as="fetch" rel="preload" href="/ai-assurance-guide/page-data/applying-assurance-techniques/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/ai-assurance-guide/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><style data-emotion-css="xwm6ft">@import url("https://fonts.googleapis.com/css2?family=Open+Sans&display=swap");@font-face{font-family:"Alte DIN 1451 Mittelschrift";src:url("static/fonts/din1451alt.ttf");}@font-face{font-family:"Alte DIN 1451 Mittelschrift";src:url("static/fonts/din1451alt_g.ttf");font-weight:bold;}*,*::after,*::before{margin:0;padding:0;box-sizing:border-box;}*::selection{background:#d0d0d5!important;}body{font-size:16px;font-family:"Open Sans",sans-serif;background-color:#f0ede3;text-rendering:optimizelegibility;}h1{font-size:32px;font-family:"Alte DIN 1451 Mittelschrift",sans-serif;color:#262445;font-weight:normal;margin-bottom:24px;}h2{font-size:24px;}h3{font-size:18px;}h4{font-size:16px;}h2,h3,h4,h5,h6{font-family:"Alte DIN 1451 Mittelschrift",sans-serif;color:#262445;margin:24px 0 16px 0;font-weight:normal;}p{color:#262445;font-size:16px;line-height:28px;margin-bottom:16px;font-weight:400;}code.inline-code{display:inline-block;vertical-align:middle;line-height:1;padding:0.2em 0.2em 0.3em 0.2em;background-color:#44475a;color:rgba(248,248,242);font-size:14px;border-radius:3px;font-feature-settings:"clig" 0,"calt" 0;font-variant:no-common-ligatures no-discretionary-ligatures no-historical-ligatures no-contextual;}a{color:#262445;font-weight:bold;}a:hover{color:#4b4788 !important;-webkit-transition:all 100ms cubic-bezier(0.4,0,0.2,1) 0s;transition:all 100ms cubic-bezier(0.4,0,0.2,1) 0s;opacity:1 !important;}blockquote{margin:0;}blockquote p{padding:1rem;background:#f5f5fa;border-radius:5px;}hr{border:0;height:0;border-top:1px solid rgba(0,0,0,0.1);border-bottom:1px solid rgba(255,255,255,0.3);}table{border-collapse:collapse;border-spacing:0;width:100%;margin-bottom:16px;margin-top:16px;color:#444;}th{background-color:#00a886;color:#ffffff;padding:12px;}td{text-align:left;padding:12px;color:#262445;}tr{background-color:#ffffff;border:solid;border-width:1px 0;border-color:#f0ede3;}tr:first-child{border-top:none;}tr:last-child{border-bottom:none;}tr:hover{background-color:#f1f1f1f1;}iframe{margin-bottom:16px;}img{max-width:100%;}ul,ol{color:#737380;padding-left:15px;margin-bottom:16px;}ul li,ol li{line-height:28px;}.gatsby-highlight{position:relative;}.gatsby-highlight .token{font-style:normal !important;}pre[class*="language-"]::before{background:#d9d7e0;border-radius:0 0 4px 4px;color:#232129;font-size:12px;font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;-webkit-letter-spacing:0.075em;-moz-letter-spacing:0.075em;-ms-letter-spacing:0.075em;letter-spacing:0.075em;line-height:1;padding:0.25rem 0.5rem;position:absolute;left:1rem;text-align:right;text-transform:uppercase;top:0;}pre[class*="language-"] code{font-family:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;font-variant:no-common-ligatures no-discretionary-ligatures no-historical-ligatures no-contextual;}pre[class~="language-js"]::before,pre[class~="language-javascript"]::before{content:"js";background:#f7df1e;}pre[class~="language-jsx"]::before{content:"jsx";background:#61dafb;}pre[class~="language-typescript"]::before,pre[class~="language-ts"]::before{content:"ts";background:#294e80;color:#fff;}pre[class~="language-tsx"]::before{content:"tsx";background:#294e80;color:#fff;}pre[class~="language-graphql"]::before{content:"GraphQL";background:#e10098;color:#fff;}pre[class~="language-html"]::before{content:"html";background:#005a9c;color:#fff;}pre[class~="language-css"]::before{content:"css";background:#ff9800;color:#fff;}pre[class~="language-mdx"]::before{content:"mdx";background:#f9ac00;color:#fff;}pre[class~="language-shell"]::before{content:"shell";}pre[class~="language-sh"]::before{content:"sh";}pre[class~="language-bash"]::before{content:"bash";}pre[class~="language-yaml"]::before{content:"yaml";background:#ffa8df;}pre[class~="language-markdown"]::before{content:"md";}pre[class~="language-json"]::before,pre[class~="language-json5"]::before{content:"json";background:linen;}pre[class~="language-diff"]::before{content:"diff";background:#e6ffed;}pre[class~="language-text"]::before{content:"text";background:#fff;}pre[class~="language-flow"]::before{content:"flow";background:#e8bd36;}</style><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><style data-emotion-css="rmvnoy">.css-rmvnoy{box-shadow:10px 0 4px -4px #888;width:20%;max-width:280px;min-width:280px;background-color:#f0ede3;position:fixed;overflow-y:auto;left:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-transition:-webkit-transform 0.5s;-webkit-transition:transform 0.5s;transition:transform 0.5s;height:100vh;}.css-rmvnoy nav{width:100%;-webkit-align-self:flex-start;-ms-flex-item-align:start;align-self:flex-start;margin-bottom:20px;-webkit-flex:1;-ms-flex:1;flex:1;}.css-rmvnoy footer{padding:24px 0 24px 30px;width:100%;}.css-rmvnoy footer p{color:#A8A8B3;font-size:12px;margin:0;}@media (max-width:780px){.css-rmvnoy{max-width:240px;min-width:240px;-webkit-transform:translate3d( -100%,0,0 );-ms-transform:translate3d( -100%,0,0 );transform:translate3d( -100%,0,0 );}}</style><aside class="css-rmvnoy e121dujv0"><style data-emotion-css="1qro3c0">.css-1qro3c0{width:100%;height:100%;max-height:134px;min-height:134px;padding:20px 0 40px;}.css-1qro3c0 a{width:100%;height:100%;padding-left:30px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><div class="css-1qro3c0 e121dujv1"><a href="/ai-assurance-guide/"><style data-emotion-css="wka7ye">.css-wka7ye{width:200px;height:110px;background-size:contain;background:url(/ai-assurance-guide/static/logo-b4e143161431caacaae66f88db431f0f.svg) center no-repeat;margin-top:30px;}</style><div class="css-wka7ye e1r98pmc0"></div></a></div><nav><style data-emotion-css="mbmci8">.css-mbmci8{list-style:none;width:100%;padding-left:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}</style><ul class="css-mbmci8 e121dujv2"><style data-emotion-css="1e420vj">.css-1e420vj{font-size:15px;width:100%;-webkit-transition:all 200ms ease-in-out;transition:all 200ms ease-in-out;padding:0 20px;}.css-1e420vj a,.css-1e420vj span{display:block;font-size:15px;color:#262445;background-color:#f0ede3;padding:4px 10px;margin:4px 0;font-weight:normal;-webkit-text-decoration:none;text-decoration:none;width:100%;height:100%;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;cursor:pointer;margin:0 auto;-webkit-transition:background-color 0.2s,color 0.2s,padding-left 0.2s;transition:background-color 0.2s,color 0.2s,padding-left 0.2s;}.css-1e420vj a svg,.css-1e420vj span svg{width:20px;height:20px;margin-right:10px;}.css-1e420vj a:not(.active-link):hover,.css-1e420vj span:not(.active-link):hover{padding-left:20px;color:#f2603b !important;}.css-1e420vj a.active-link,.css-1e420vj span.active-link,.css-1e420vj a.active-link:hover,.css-1e420vj span.active-link:hover{color:white !important;background-color:#f2603b;}</style><li class="css-1e420vj e121dujv4"><a href="/ai-assurance-guide/">Home</a></li><style data-emotion-css="1i6zgr9">.css-1i6zgr9{padding-left:30px;width:100%;text-transform:uppercase;font-size:13px;font-weight:bold;margin-top:20px;color:#00a886;-webkit-letter-spacing:0.142em;-moz-letter-spacing:0.142em;-ms-letter-spacing:0.142em;letter-spacing:0.142em;font-family:"Alte DIN 1451 Mittelschrift",sans-serif;}</style><li class="css-1i6zgr9 e121dujv3">Background</li><style data-emotion-css="1ggkth3">.css-1ggkth3{list-style:none;width:100%;padding-left:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;margin-top:5px;}</style><ul class="css-1ggkth3 e121dujv5"><li class="css-1e420vj e121dujv4"><a href="/ai-assurance-guide/benefits-and-risks">Artificial Intelligence, benefits and risks</a></li><li class="css-1e420vj e121dujv4"><a href="/ai-assurance-guide/trust">The need for trust in AI systems</a></li><li class="css-1e420vj e121dujv4"><a href="/ai-assurance-guide/what-is-assurance">What is AI assurance and why do we need it?</a></li><li class="css-1e420vj e121dujv4"><a href="/ai-assurance-guide/governance">What is the role of AI assurance in AI governance?</a></li><li class="css-1e420vj e121dujv4"><a href="/ai-assurance-guide/needs-and-responsibilities">Needs and Responsibilities for AI Assurance?</a></li></ul><li class="css-1i6zgr9 e121dujv3">AI assurance engagements</li><ul class="css-1ggkth3 e121dujv5"><li class="css-1e420vj e121dujv4"><a href="/ai-assurance-guide/five-elements">The 5 elements of an assurance engagement</a></li><li class="css-1e420vj e121dujv4"><a href="/ai-assurance-guide/examples">Examples from other assurance ecosystems</a></li><li class="css-1e420vj e121dujv4"><a href="/ai-assurance-guide/technical-standards">The role of technical standards in AI assurance</a></li><li class="css-1e420vj e121dujv4"><a href="/ai-assurance-guide/challenges">Challenges for AI assurance</a></li></ul><li class="css-1i6zgr9 e121dujv3">Delivering AI assurance</li><ul class="css-1ggkth3 e121dujv5"><li class="css-1e420vj e121dujv4"><a href="/ai-assurance-guide/techniques">Techniques for assuring AI systems</a></li><li class="css-1e420vj e121dujv4"><a href="/ai-assurance-guide/subject-matter">Assuring different subject matter</a></li><li class="css-1e420vj e121dujv4"><a href="/ai-assurance-guide/lifecycle">Assurance across the AI system lifecycle</a></li></ul><li class="css-1i6zgr9 e121dujv3">The AI assurance toolkit</li><ul class="css-1ggkth3 e121dujv5"><li class="css-1e420vj e121dujv4"><a href="/ai-assurance-guide/applying-assurance-techniques">Applying assurance techniques to AI</a></li><li class="css-1e420vj e121dujv4"><a href="/ai-assurance-guide/independence">The role of independence in assuring AI</a></li><li class="css-1e420vj e121dujv4"><a href="/ai-assurance-guide/promise-and-limits">The promise and limits of assurance</a></li><li class="css-1e420vj e121dujv4"><a href="/ai-assurance-guide/case-studies">Assurance case studies and examples</a></li></ul><li class="css-1i6zgr9 e121dujv3">Privacy</li><ul class="css-1ggkth3 e121dujv5"><li class="css-1e420vj e121dujv4"><a href="/ai-assurance-guide/cookie-policy">Cookie Policy</a></li><li class="css-1e420vj e121dujv4"><a href="/ai-assurance-guide/privacy-notice">Privacy Notice</a></li></ul></ul></nav><footer><p></p></footer></aside><div class="headroom-wrapper"><div style="position:relative;top:0;left:0;right:0;z-index:1;-webkit-transform:translate3D(0, 0, 0);-ms-transform:translate3D(0, 0, 0);transform:translate3D(0, 0, 0)" class="headroom headroom--unfixed"><style data-emotion-css="1eexyoi">.css-1eexyoi{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:60px;padding:0 24px;background:#262445;color:#fff;-webkit-transition:-webkit-transform 0.5s;-webkit-transition:transform 0.5s;transition:transform 0.5s;-webkit-transform:translate3d( 0,0,0 );-ms-transform:translate3d( 0,0,0 );transform:translate3d( 0,0,0 );}.css-1eexyoi h2{margin:0;border:none;padding:0;font-size:18px;color:#fff;}.css-1eexyoi button{border:none;background:#fff;cursor:pointer;margin-right:16px;}@media (min-width:780px){.css-1eexyoi{display:none;}}</style><header class="css-1eexyoi e1jc2v0p0"><button aria-label="Open sidebar" type="button"><div style="color:white;background:#262445"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" size="23" height="23" width="23" xmlns="http://www.w3.org/2000/svg"><path d="M32 96v64h448V96H32zm0 128v64h448v-64H32zm0 128v64h448v-64H32z"></path></svg></div></button><h2>AI assurance guide</h2></header></div></div><style data-emotion-css="f2zvwr">.css-f2zvwr{padding-left:280px;-webkit-transition:-webkit-transform 0.5s;-webkit-transition:transform 0.5s;transition:transform 0.5s;}@media (max-width:780px){.css-f2zvwr{padding-left:0;-webkit-transform:translate3d( 0,0,0 );-ms-transform:translate3d( 0,0,0 );transform:translate3d( 0,0,0 );}}</style><div class="css-f2zvwr ehr0lei2"><style data-emotion-css="844t05">.css-844t05{padding:40px 0 0 40px;}@media (max-width:780px){.css-844t05{padding:24px 0 0 24px;}}</style><h1 class="css-844t05 ehr0lei3">Applying assurance techniques to AI</h1><style data-emotion-css="o1lccg">.css-o1lccg{padding:0 40px;height:100%;display:inline-block;}@media (max-width:780px){.css-o1lccg{padding:24px 24px 48px 24px;}}</style><main class="css-o1lccg ehr0lei0"><style data-emotion-css="e1d30b">.css-e1d30b{width:100%;min-width:75%;max-width:90%;}@media (max-width:1200px){.css-e1d30b{min-width:100%;max-width:100%;}}</style><div class="css-e1d30b ehr0lei1"><p>For AI, a number of different aspects of an AI system and the broader context in which it is deployed need to be assured. While there are many lists of objectives and risks for AI, there is an emerging <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3518482">consensus</a> about the types of risk and harm, and aspects of performance that need to be understood.</p><p>Potential subject matter for assuring AI systems:</p><ul><li>Intended use - is the intended use of the system beneficial and appropriate for the type of system?</li><li>Accuracy - Is the system accurate and effective in achieving its intended goals?</li><li>Robustness - Is the system’s performance consistent across a variety of inputs or in different conditions?</li><li>Unfair bias - Does the system perpetuate unfair bias?</li><li>Explainability / interpretability - can the reasons behind a system’s decisions or predictions be explained to those affected?</li><li>Security - Is the system vulnerable to cyber attack or malicious use?</li><li>Societal impact - Could the operation of the system result in negative consequences for people and society?</li><li>Privacy - Does the system ensure individuals privacy rights are respected and protected?</li><li>Human rights - Does the system pose a risk to individuals’ human rights?</li><li>Management processes and controls - Are the appropriate processes and controls in place to manage risks in the development or deployment of the system?</li></ul><p>The levels at which these subject matter could be assessed:</p><ul><li>Input data</li><li>Test performance</li><li>Real world outcomes</li><li>Process</li><li>Organisations</li><li>Developers</li></ul><p>Considering the wide range of potential subject matter for AI Assurance, there is an important challenge to ensure that suitable assurance techniques are adopted, reflecting the characteristics of each subject matter. Deciding on this will involve active participation from across the ecosystem. Government, regulators and standards bodies will need to work closely with industry, academia and civil society to set suitable assurable requirements for AI systems.</p><p>For example, the potential societal impacts of an AI system before deployment need to be measured differently to how we would measure the accuracy of an AI system in performing a specific task. In the latter case the accuracy of an AI system can be established with a relatively high level of certainty using a quantitative metric. This is notwithstanding the potential for <a href="https://www.ibm.com/uk-en/cloud/watson-studio/drift">model drift</a>, which will require ongoing, live testing to ensure the system remains accurate across its lifecycle. In contrast, when thinking about potential societal impacts of an AI system, assessors need to establish potential impacts resulting from a system which would not occur in a counterfactual world in which the technology was not being developed.</p><p>This process involves qualitative analysis such as public engagement, futures thinking and consideration of public values. Impact assessments provide assurance to the extent that proper procedures have been followed to identify, mitigate and assign responsibility for impacts - not that all possible impacts have in fact been mitigated (this is an unachievable level of certainty). In contrast, the accuracy of a model against a predetermined standard can be assured to a far greater degree of certainty. While the potential for model drift might decrease this certainty when the system is deployed and used, this can be mitigated via ongoing accuracy testing.</p><p>We think that because of these strengths and weaknesses, the assurance techniques listed in section 2.1 provide a useful toolbox of complementary techniques that should be considered when trying to establish trust in AI. However it is important that assurance techniques are coherently and deliberately chosen to suit the nature of the subject matter.</p><h2 id="table-mapping-example-ai-assurance-subject-matter-to-suitable-assurance-techniques" style="position:relative"><a href="#table-mapping-example-ai-assurance-subject-matter-to-suitable-assurance-techniques" aria-label="table mapping example ai assurance subject matter to suitable assurance techniques permalink" class="anchor before"><svg aria-hidden="true" focusable="false" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Table: Mapping example AI assurance subject matter to suitable assurance techniques</h2><table><thead><tr><th align="left">Subject matter</th><th align="left">Key characteristics</th><th align="left">Applicable assurance techniques</th><th align="left">Explanation</th></tr></thead><tbody><tr><td align="left">Accuracy</td><td align="left">Measurable against quantitative metrics. Can be assessed against explicit and objective standards to a relatively high degree of certainty. However, subjective judgement required around what measures of accuracy e.g. precision, sensitivity and benchmarks for acceptable levels of accuracy.</td><td align="left">Performance testing, Certification based on performance level, compliance audit for performance testing documentation.</td><td align="left"><p>The performance of an AI system e.g. its accuracy in relation to a certain task can be measured against a specific quantitative metric with a relatively high level of certainty and objectivity.</p><p>To maintain a high-level of certainty and reliability about the accuracy of a system when operating in the wild, the user or an independent assurance provider will need to carry out ongoing monitoring to account for model drift. This is needed to ensure that the model remains accurate as the properties of the input data or target variables change over time.</p><p>However, there are several different ways of measuring <a href="https://ico.org.uk/about-the-ico/news-and-events/ai-blog-accuracy-of-ai-system-outputs-and-performance-measures/">statistical accuracy</a>. For example, you could measure the precision (the percentage of cases identified as positive that are in fact positive)​​, or you could measure the sensitivity of an AI system (the percentage of all cases that are positive and that are identified as such).</p><p>Often, there are trade-offs between these different measurements: improving the precision of the model could lead to the reduction of sensitivity and vice-versa. In these cases, it then becomes a subjective decision about which accuracy metric to prioritise.</p><p>Due to the tradeoffs between precision and sensitivity, decisions about accuracy will need to be context specific. For example, if using an AI system in a medical imaging device to detect potentially cancerous tumours, sensitivity would be more important. While false positives might risk unwanted distress, false negatives could have deadly consequences.</p></td></tr><tr><td align="left">Bias</td><td align="left"><a href="https://www.gov.uk/government/publications/cdei-publishes-review-into-bias-in-algorithmic-decision-making/main-report-cdei-review-into-bias-in-algorithmic-decision-making">Bias</a> can be measured quantitatively e.g. measuring an adverse impact ratio. But deciding which measure of bias is appropriate and the level of bias that counts as ‘unfair’ in a specific context are both qualitative judgements.</td><td align="left">Bias audit, Impact assessment is also required to understand the relative severity of potential harms resulting from algorithmic bias.</td><td align="left"><p>If assessing an AI system to detect unfair bias, first an appropriate metric needs to be chosen for the use context e.g. demographic parity. Choosing an appropriate metric is, in the first place, subjective. A bias audit can then be carried out by an internal or external team to determine the performance of the algorithm in relation to the chosen metric. This test alone cannot determine whether or not the algorithm is unfairly biased - this requires considering and setting a subjective benchmark for the level of demographic parity that is acceptable. Assurance can be provided to the degree that, according to the chosen benchmark for demographic parity, the algorithm is not unfairly biased.</p><p>In this case, what is being assured is that an appropriate metric for fairness has been chosen and based upon the results of the audit, the responsible party has determined that the algorithm is not biased according to a known benchmark.</p></td></tr><tr><td align="left">Societal impacts e.g. Human rights Impacts</td><td align="left">Can be assessed qualitatively with possible quantitative estimates of scope/scale or risk. However societal impacts cannot be observed and are inherently uncertain.</td><td align="left">Impact assessments</td><td align="left"><p> <a href="https://datasociety.net/library/assembling-accountability-algorithmic-impact-assessment-for-the-public-interest/">Impact assessments</a> are used to identify, manage and mitigate the potential harms to society e.g. discrimination, resulting from or linked to the impacts of an AI system e.g. a disparate error rate.</p><p>Impacts need to be assessed as proxies for future harms so that responsibility can be appropriately assigned to mitigate these impacts. Impacts cannot be assessed quantitatively due to future uncertainty and the inherent limitations on quantifying harms. Therefore impact assessment must include a qualitative assessment of impacts, including stakeholder engagement and assessing potential impacts in relation to established values. Quantitative analysis of the projected scope and scale of impacts may be used alongside qualitative analysis.</p><p>Impact assessment cannot assure with certainty that harms will not occur, or that the specified impacts are the only impacts of concern. Assurance can be provided to the degree that known impacts have been identified, accountabilities have been identified for these impacts and mitigation strategies put in place. </p><p>In this case, what is being assured is that an agreed upon, formalised process for assessing potential societal harms has taken place, and that appropriate accountabilities have been assigned and mitigation strategies put in place.</p><p>An important but ambiguous question for the assessor is, what impacts are in scope? There are different ways that a line could be drawn around the impacts from deploying an AI system. While direct and immediate harms to rights and freedoms would certainly be in scope, it is unclear whether more distant and indirect harms would need to be considered. For example, job losses resulting from automation over the following decades. Deploying an AI system may contribute to this harm, but might not be a key factor. </p></td></tr><tr><td align="left">Privacy and data protection</td><td align="left">Data protection rating can be measured qualitatively.</td><td align="left">Data Protection Impact Assessment, Privacy Impact assessment, privacy audit for model inferences.</td><td align="left"><p>When assessing the privacy of an AI system throughout its lifecycle, the assessor needs to focus on at least two distinct subject matter areas. Firstly, the data component which can maintain sensitive, personally identifying information, or may be obtained unlawfully. The assessor can carry out a <a href="https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/accountability-and-governance/data-protection-impact-assessments/">Protection Impact Assessment</a>Data (DPIA), providing a qualitative rating based on the perceived level of data protection.</p><p>Secondly model inferences need to be assessed as to whether they identify data subjects or groups. This makes measuring and assessing algorithmic privacy partially dependent upon the explainability and interpretability of the AI system. </p><p>As part of a complex system, the data-algorithm interaction must also be assessed to assure against vulnerabilities arising from the relationship between the data and algorithm. </p><p>System privacy is also an important subject matter for assuring the security of the system. For example, assessing the system for privacy vulnerabilities leaving it open to adversarial or malicious uses.</p><p>It is important to note that in some cases, privacy and data protection diverge as subject matter for assurance. For example, model inferences or predictions may impact an individual’s or <a href="https://www.stiftung-nv.de/sites/default/files/group-privacy-2017-authors-draft-manuscript.pdf">group’s privacy</a> without involving personal data and engaging data protection. Similarly, data protection regulation contains articles on the right to an explanation for an algorithmic decision, which is not necessarily privacy related. </p></td></tr><tr><td align="left">Robustness</td><td align="left">Model specification can be quantitatively assessed. Qualitative assessment required to establish acceptable benchmarks for robustness for a specific use case.</td><td align="left">Performance testing and formal verification required to test the specification of the model - testing the relationship that must hold between the inputs and outputs of the system across variations in input data.</td><td align="left"><p>The <a href="https://www.brookings.edu/techstream/why-robustness-is-key-to-deploying-ai/">robustness</a> of an AI system corresponds to how effectively the system can be deemed safe and reliable across variations in the input variables used to make predictions.</p><p>Evidence about robustness is required to ensure AI systems will be accurate and safe when faced with unexpected events that occur naturally during operation or if they are tampered with by a malicious actor.</p><p>Different types of AI systems will have different <a href="https://arxiv.org/pdf/2108.06159.pdf">robustness properties</a> and different robustness bounds (acceptable levels of robustness). For example, a computer vision system for an autonomous vehicle will require a high level of robustness due to the serious potential harms of malfunction. Robustness properties for a computer vision system in an autonomous vehicle include: Image noise, deviations from optimal image conditions e.g. a branch partially covering a sign; geometric transformations e.g. road signs shifted at different angles; colour transformations e.g. differences in performance in daylight and nighttime.</p><p>Performance testing metrics and formal verification can be used to test and quantify a system’s robustness properties. However, judgement is required to decide on the relevant robustness properties and what robustness scores are acceptable. </p></td></tr><tr><td align="left">Explainability / interpretability</td><td align="left">Explainability / interpretability dependent on characteristics of an AI system</td><td align="left">Technical AI explainability toolkits, ICO/Turing Explaining Decision made with AI Guidance</td><td align="left"><p>Explainability refers to the extent to which an AI system provides decisions or suggestions that can be understood by their users and developers. Interpretability refers to how easily cause and effect can be determined for the outputs of a system. </p><p>Acceptable levels of explainability and interpretability vary depending on whether the use case is high or low risk. In high risk cases, drawing unjustified conclusions from data could cause serious harm.Explainability can be either an inherent characteristic of an AI system or it can be approximated by other methods. This latter type of explainability can be important for so-called ‘black box’ models like artificial neural networks. </p><p>Explainability and interpretability are not always ends in themselves for assurance but enable systems to be audited and understood to come to conclusions about their trustworthiness. However, explainability and interpretability of a model are important in their own right for developers to check their models <a href="https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-01332-6">beyond just their performance</a>, to rule out that a system is making predictions based upon meta-data rather than the specified input data. A famous example is of an algorithm that distinguished between <a href="https://arxiv.org/abs/2003.07631">wolves and huskies</a>, yet this was driven entirely by whether there was a snowy background or not.</p><p>A further dimension of explainability concerns the right to an explanation. For example, the transparency principle laid out in articles 13-25 in the GDPR requires AI developers and controllers to provide ‘meaningful information about the logic involved’ in a model or algorithm. This problem relates to data protection, but not privacy.</p><p>These articles set out a <a href="https://academic.oup.com/idpl/article/7/4/233/4762325">right to an explanation</a>. The ICO/Turing Guidance on Explaining Decisions made with AI sets out requirements for AI developers or controllers for providing meaningful information to explain a decision:<ul><li>The type of information collected or used in creating the profile or making the automated decision;</li><li>why this information is relevant; and</li><li>what the likely impact is going to be/how it&#x27;s likely to affect data subjects</li></ul></p><p>For example, if a credit scoring organisation uses an automated process to assess a customer’s creditworthiness. For a traditional credit score, they will need to process data relating to the customer’s credit history, such as payment history, length of credit history, number of credit accounts. An alternative credit scoring agency may additionally use personality data or social media data.</p><p>The credit scoring agency will need to explain what type of data is collected to build the credit profile, why this data is relevant to prediction the customers credit score, and the likely impact that such a score will have on the customer I.e. being approved for or denied credit.</p></td></tr><tr><td align="left">Intended use / benefit</td><td align="left">The scope and acceptable use cases for an AI system need to be qualitatively assessed based on the function of the AI system and the goals for using it. This assessment relies on evidence from quantitative assessment of accuracy, robustness, bias to determine use case suitability.</td><td align="left">Performance testing required for assessing robustness and bias audit required for assessing fairness of a system. Impact assessment required for understanding the potential impacts of using an AI system in a specific use case.</td><td align="left"><p>To build trust in whether an AI system is suitable for the context or task for which it is being used requires evaluating evidence about it’s performance and robustness and considering the potential societal impacts or safety issues the system might cause if it were to be deployed.</p><p>For example, the robustness, accuracy, fairness and explainability of an AI system will need to be very high if its intended use is in a medical imaging device. Because of the specific, complex and high-risk nature of medical imaging, an AI system could be certified for a very narrow set of uses or even a single task.</p><p>In lower risk applications an AI system may be certified for use in a number of different contexts and across a number of different tasks.</p></td></tr><tr><td align="left">Management systems and internal controls</td><td align="left">Management systems and internal controls need to be assessed qualitatively but can be assessed objectively against explicit standards and criteria e.g. <a href="https://www.iso.org/standard/62085.html">ISO 9001 Quality Management standards</a>.</td><td align="left">Conformtity assessments, including systems and process audits required to assess conformity of management processes including internal controls and quality management systems with standards, guidelines and regulations.</td><td align="left"><p><a href="https://www.iso.org/management-system-standards.html">Management systems</a> and internal controls cover both technical and non technical aspects of AI system development, deployment and use. Management systems are used to assure complex socio-technical processes including quality and risk management.</p><p>Management system standards provide explicit requirements against which organisational management can be assessed and its trustworthiness understood by assurance users. </p></td></tr><tr><td align="left">Security</td><td align="left">AI security can be assured at <a href="https://www.isaca.org/resources/isaca-journal/past-issues/2012/fundamental-concepts-of-it-security-assurance#f26">three levels</a>: 1) Assessment of the product, system or service 2) Assessment of the development process of the product, system or service 3) assessment of the environment, such as the security management and governance.</td><td align="left"><p>Security compliance audits, threat modelling and identification, Red teaming (penetration testing), product/service performance evaluation</p><p>Available examples of technical standards:<ul><li>Applicable to products: ISO/IEC 15408 Information technology—Security techniques—Evaluation criteria for IT security</li><li>Applicable to processes: ISO/IEC 21827 :2002 Information technology— Systems Security Engineering—Capability Maturity Model</li><li>Applicable to security management: ISO/IEC 27001 Information technology—Security techniques—Information security management systems— Requirements</li></ul></p></td><td align="left">ISO/IEC TR 15443 defines these three high-level approaches as follows:<ol><li>For the assessment of a product, system, service - In this case, assurance methods examine the product, system or service and its associated security design documentation independent of the development processes.</li><li>Assessment of a process involves examining the organisational processes used in the production and operation of the product, system or service throughout its life cycle (i.e., development, deployment, delivery, testing, maintenance, disposal). Assurance is gained through the inference that the processes implemented by people affect the quality of the development and implementation of the product, system or service and, therefore, yield security assurance.</li><li>Assessment of the environment involves an examination of the environmental factors that contribute to the quality of the processes and the production of the product, system or service. This type of assurance does not examine a deliverable or process directly. These factors assessed include personnel and physical facilities (e.g., development, production, delivery, operation).</li></ol></td></tr></tbody></table><div><p style="font-size:12px"><i>All content is available under the<!-- --> <a href="https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/">Open Government License v3.0</a> <!-- -->except where otherwise stated.</i></p></div><style data-emotion-css="1rfih1g">.css-1rfih1g{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding:48px 0;width:100%;}@media (max-width:780px){.css-1rfih1g{-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}}</style><section class="css-1rfih1g e1jikabl0"><style data-emotion-css="aijdyu">.css-aijdyu{-webkit-transition:all 200ms;transition:all 200ms;}.css-aijdyu a{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-text-decoration:none;text-decoration:none;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:100%;}.css-aijdyu a svg{width:25px;height:25px;color:#737380;margin-right:16px;}.css-aijdyu a p{-webkit-letter-spacing:0.142em;-moz-letter-spacing:0.142em;-ms-letter-spacing:0.142em;letter-spacing:0.142em;text-transform:uppercase;font-size:12px;margin:0;color:#999;}.css-aijdyu a h3{color:#737380;border:none;margin:0;padding:0;font-weight:normal;font-size:16px;}.css-aijdyu:hover{opacity:0.8;}.css-aijdyu:hover a svg{opacity:0.8;}@media (max-width:780px){.css-aijdyu{width:100%;margin-bottom:16px;}.css-aijdyu a{-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;}}</style><div class="css-aijdyu e1jikabl1"><a href="/ai-assurance-guide/lifecycle"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M15.41 16.09l-4.58-4.59 4.58-4.59L14 5.5l-6 6 6 6z"></path></svg><div><p>Prev</p><h3>Assurance across the AI system lifecycle</h3></div></a></div><style data-emotion-css="1novzkq">.css-1novzkq{-webkit-transition:all 200ms;transition:all 200ms;margin-left:auto;}.css-1novzkq a{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-text-decoration:none;text-decoration:none;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:100%;height:100%;}.css-1novzkq a svg{width:25px;height:25px;color:#737380;margin-left:16px;}.css-1novzkq a p{-webkit-letter-spacing:0.142em;-moz-letter-spacing:0.142em;-ms-letter-spacing:0.142em;letter-spacing:0.142em;text-transform:uppercase;font-size:12px;margin:0;color:#999;}.css-1novzkq a h3{color:#737380;border:none;margin:0;padding:0;font-weight:normal;font-size:16px;}.css-1novzkq:hover{opacity:0.8;}.css-1novzkq:hover a svg{opacity:0.8;}@media (max-width:780px){.css-1novzkq{width:100%;}.css-1novzkq a{-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;}}</style><div class="css-1novzkq e1jikabl1"><a href="/ai-assurance-guide/independence"><div><p>Next</p><h3>The role of independence in assuring AI</h3></div><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M8.59 16.34l4.58-4.59-4.58-4.59L10 5.75l6 6-6 6z"></path></svg></a></div></section></div></main></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script async="" src="https://www.googletagmanager.com/gtag/js?id=UA-173229929-3"></script><script>
        function gaOptout(){document.cookie=disableStr+'=true; expires=Thu, 31 Dec 2099 23:59:59 UTC;path=/',window[disableStr]=!0}var gaProperty='UA-173229929-3',disableStr='ga-disable-'+gaProperty;document.cookie.indexOf(disableStr+'=true')>-1&&(window[disableStr]=!0);
         window.dataLayer = window.dataLayer || [];
         function gtag(){dataLayer.push(arguments);}
        </script><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/applying-assurance-techniques/";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"app":["/app-39b77a34e7ba1018ce1d.js"],"component---cache-caches-gatsby-plugin-offline-app-shell-js":["/component---cache-caches-gatsby-plugin-offline-app-shell-js-e5cb9e0c77a823b86dc2.js"],"component---node-modules-rocketseat-gatsby-theme-docs-core-src-templates-docs-query-js":["/component---node-modules-rocketseat-gatsby-theme-docs-core-src-templates-docs-query-js-61d59f35b4662c9277ef.js"],"component---node-modules-rocketseat-gatsby-theme-docs-core-src-templates-homepage-query-js":["/component---node-modules-rocketseat-gatsby-theme-docs-core-src-templates-homepage-query-js-3357b8bc0cb619e884dd.js"],"component---src-pages-404-js":["/component---src-pages-404-js-c48051cf54f762f1e833.js"]};/*]]>*/</script><script src="/ai-assurance-guide/component---node-modules-rocketseat-gatsby-theme-docs-core-src-templates-docs-query-js-61d59f35b4662c9277ef.js" async=""></script><script src="/ai-assurance-guide/a7629950cb7a18c7593baf47d0acfe798f1cddc4-bc84a563ac4ce2a5b404.js" async=""></script><script src="/ai-assurance-guide/252f366e-e522de43b84fd218bea8.js" async=""></script><script src="/ai-assurance-guide/5e2a4920-b71d4aaefb263d56b92c.js" async=""></script><script src="/ai-assurance-guide/545f34e4-5ad47dd60d402bdffae8.js" async=""></script><script src="/ai-assurance-guide/app-39b77a34e7ba1018ce1d.js" async=""></script><script src="/ai-assurance-guide/f0e45107-1b350d56f045fa1c0ff3.js" async=""></script><script src="/ai-assurance-guide/styles-24c541b6ac347bae38f1.js" async=""></script><script src="/ai-assurance-guide/framework-95b680cb3190aa9bfe59.js" async=""></script><script src="/ai-assurance-guide/webpack-runtime-076fa458eaac626e98e4.js" async=""></script></body></html>