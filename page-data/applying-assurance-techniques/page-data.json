{"componentChunkName":"component---node-modules-rocketseat-gatsby-theme-docs-core-src-templates-docs-query-js","path":"/applying-assurance-techniques/","result":{"data":{"mdx":{"id":"b7ed2d36-fb58-5f88-a282-c462bdaa7804","excerpt":"For AI, a number of different aspects of an AI system and the broader context in which it is deployed need to be assured. While there are many lists ofâ€¦","fields":{"slug":"/applying-assurance-techniques/"},"frontmatter":{"title":"Applying assurance techniques to AI","description":null,"image":null,"disableTableOfContents":true},"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Applying assurance techniques to AI\",\n  \"disableTableOfContents\": true\n};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component \" + name + \" was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(CookieBanner, {\n    mdxType: \"CookieBanner\"\n  }), mdx(\"p\", null, \"For AI, a number of different aspects of an AI system and the broader context in which it is deployed need to be assured. While there are many lists of objectives/risks for AI, there is an emerging consensus about the types of risk/harm/performance that need to be understood.\"), mdx(\"p\", null, \"Potential subject matter for assuring AI systems:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Intended use - is the intended use of the system beneficial and appropriate for the type of system?\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Accuracy - Is the system accurate and effective in achieving its intended goals?\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Robustness - Is the system\\u2019s performance consistent across a variety of inputs or in different conditions?\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Unfair bias - Does the system perpetuate unfair bias?\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Explainability / interpretability - can the reasons behind a system\\u2019s decisions or predictions be explained to those affected?\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Security - Is the system vulnerable to cyber attack or malicious use?\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Societal impact - Could the operation of the system result in negative consequences for people and society?\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Privacy - Does the system ensure individuals privacy rights are respected and protected?\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Human rights - Does the system pose a risk to individuals\\u2019 human rights?\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Management processes and controls - Are the appropriate processes and controls in place to manage risks in the development or deployment of the system?\")), mdx(\"p\", null, \"The levels at which these subject matter could be assessed:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Input data\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Test performance\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Real world outcomes\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Process\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Organisations\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Developers\")), mdx(\"p\", null, \"Considering the wide range of potential subject matter for AI Assurance, there is an important challenge to ensure that suitable assurance techniques are adopted, reflecting the characteristics of each subject matter. Deciding on this will involve active participation from across the ecosystem. Government, regulators and standards bodies will need to work closely with industry, academia and civil society to set suitable assurable requirements for AI systems.\"), mdx(\"p\", null, \"For example, the potential societal impacts of an AI system before deployment need to be measured differently to how we would measure the accuracy of an AI system in performing a specific task. In the latter case the accuracy of an AI system can be established with a relatively high level of certainty using a quantitative metric. This is notwithstanding the potential for model drift, which will require ongoing, live testing to ensure the system remains accurate across its lifecycle. In contrast, when thinking about potential societal impacts of an AI system, assessors need to establish potential impacts resulting from a system which would not occur in a counterfactual world in which the technology was not being developed.\"), mdx(\"p\", null, \"This process involves qualitative analysis such as public engagement, futures thinking and consideration of public values. Impact assessments provide assurance to the extent that proper procedures have been followed to identify, mitigate and assign responsibility for impacts - not that all possible impacts have in fact been mitigated (this is an unachievable level of certainty). In contrast, the accuracy of a model against a predetermined standard can be assured to a far greater degree of certainty. While the potential for model drift might decrease this certainty when the system is deployed and used, this can be mitigated via ongoing accuracy testing.\"), mdx(\"p\", null, \"We think that because of these strengths and weaknesses, the assurance techniques listed in section 2.1 provide a useful toolbox of complementary techniques that should be considered when trying to establish trust in AI. However it is important that assurance techniques are coherently and deliberately chosen to suit the nature of the subject matter.\"), mdx(\"h2\", {\n    \"id\": \"table-mapping-example-ai-assurance-subject-matter-to-suitable-assurance-techniques\",\n    \"style\": {\n      \"position\": \"relative\"\n    }\n  }, mdx(\"a\", _extends({\n    parentName: \"h2\"\n  }, {\n    \"href\": \"#table-mapping-example-ai-assurance-subject-matter-to-suitable-assurance-techniques\",\n    \"aria-label\": \"table mapping example ai assurance subject matter to suitable assurance techniques permalink\",\n    \"className\": \"anchor before\"\n  }), mdx(\"svg\", _extends({\n    parentName: \"a\"\n  }, {\n    \"aria-hidden\": \"true\",\n    \"focusable\": \"false\",\n    \"height\": \"16\",\n    \"version\": \"1.1\",\n    \"viewBox\": \"0 0 16 16\",\n    \"width\": \"16\"\n  }), mdx(\"path\", _extends({\n    parentName: \"svg\"\n  }, {\n    \"fillRule\": \"evenodd\",\n    \"d\": \"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"\n  })))), \"Table: Mapping example AI assurance subject matter to suitable assurance techniques\"), mdx(\"table\", null, mdx(\"thead\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"thead\"\n  }, mdx(\"th\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Subject matter\"), mdx(\"th\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Key characteristics\"), mdx(\"th\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Applicable assurance techniques\"), mdx(\"th\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Explanation\"))), mdx(\"tbody\", {\n    parentName: \"table\"\n  }, mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Accuracy\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Measurable against quantitative metrics. Can be assessed against explicit and objective standards to a relatively high degree of certainty. However, subjective judgement required around what measures of accuracy e.g. precision, sensitivity and benchmarks for acceptable levels of accuracy.\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Performance testing, Certification based on performance level, compliance audit for performance testing documentation.\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), mdx(\"p\", null, \"The performance of an AI system e.g. its accuracy in relation to a certain task can be measured against a specific quantitative metric with a relatively high level of certainty and objectivity.\"), mdx(\"p\", null, \"To maintain a high-level of certainty and reliability about the accuracy of a system when operating in the wild, the user or an independent assurance provider will need to carry out ongoing monitoring to account for model drift. This is needed to ensure that the model remains accurate as the properties of the input data or target variables change over time.\"), mdx(\"p\", null, \"However, there are several different ways of measuring statistical accuracy. For example, you could measure the precision (the percentage of cases identified as positive that are in fact positive)\\u200B\\u200B, or you could measure the sensitivity of an AI system (the percentage of all cases that are positive and that are identified as such).\"), mdx(\"p\", null, \"Often, there are trade-offs between these different measurements: improving the precision of the model could lead to the reduction of sensitivity and vice-versa. In these cases, it then becomes a subjective decision about which accuracy metric to prioritise.\"), mdx(\"p\", null, \"Due to the tradeoffs between precision and sensitivity, decisions about accuracy will need to be context specific. For example, if using an AI system in a medical imaging device to detect potentially cancerous tumours, sensitivity would be more important. While false positives might risk unwanted distress, false negatives could have deadly consequences.\"))), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Bias\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Bias can be measured quantitatively e.g. measuring an adverse impact ratio. But deciding which measure of bias is appropriate and the level of bias that counts as \\u2018unfair\\u2019 in a specific context are both qualitative judgements.\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Bias audit, Impact assessment is also required to understand the relative severity of potential harms resulting from algorithmic bias.\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), mdx(\"p\", null, \"If assessing an AI system to detect unfair bias, first an appropriate metric needs to be chosen for the use context e.g. demographic parity. Choosing an appropriate metric is, in the first place, subjective. A bias audit can then be carried out by an internal or external team to determine the performance of the algorithm in relation to the chosen metric. This test alone cannot determine whether or not the algorithm is unfairly biased - this requires considering and setting a subjective benchmark for the level of demographic parity that is acceptable. Assurance can be provided to the degree that, according to the chosen benchmark for demographic parity, the algorithm is not unfairly biased.\"), mdx(\"p\", null, \"In this case, what is being assured is that an appropriate metric for fairness has been chosen and based upon the results of the audit, the responsible party has determined that the algorithm is not biased according to a known benchmark.\"))), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Societal impacts e.g. Human rights Impacts\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Can be assessed qualitatively with possible quantitative estimates of scope/scale or risk. However societal impacts cannot be observed and are inherently uncertain.\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Impact assessments\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), mdx(\"p\", null, \"Impact assessments are used to identify, manage and mitigate the potential harms to society e.g. discrimination, resulting from or linked to the impacts of an AI system e.g. a disparate error rate.\"), mdx(\"p\", null, \"Impacts need to be assessed as proxies for future harms so that responsibility can be appropriately assigned to mitigate these impacts. Impacts cannot be assessed quantitatively due to future uncertainty and the inherent limitations on quantifying harms. Therefore impact assessment must include a qualitative assessment of impacts, including stakeholder engagement and assessing potential impacts in relation to established values. Quantitative analysis of the projected scope and scale of impacts may be used alongside qualitative analysis.\"), mdx(\"p\", null, \"Impact assessment cannot assure with certainty that harms will not occur, or that the specified impacts are the only impacts of concern. Assurance can be provided to the degree that known impacts have been identified, accountabilities have been identified for these impacts and mitigation strategies put in place. \"), mdx(\"p\", null, \"In this case, what is being assured is that an agreed upon, formalised process for assessing potential societal harms has taken place, and that appropriate accountabilities have been assigned and mitigation strategies put in place.\"), mdx(\"p\", null, \"An important but ambiguous question for the assessor is, what impacts are in scope? There are different ways that a line could be drawn around the impacts from deploying an AI system. While direct and immediate harms to rights and freedoms would certainly be in scope, it is unclear whether more distant and indirect harms would need to be considered. For example, job losses resulting from automation over the following decades. Deploying an AI system may contribute to this harm, but might not be a key factor. \"))), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Privacy and data protection\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Data protection rating can be measured qualitatively.\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Data Protection Impact Assessment, Privacy Impact assessment, privacy audit for model inferences.\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), mdx(\"p\", null, \"When assessing the privacy of an AI system throughout its lifecycle, the assessor needs to focus on at least two distinct subject matter areas. Firstly, the data component which can maintain sensitive, personally identifying information, or may be obtained unlawfully. The assessor can carry out a Data Protection Impact Assessment (DPIA), providing a qualitative rating based on the perceived level of data protection.\"), mdx(\"p\", null, \"Secondly model inferences need to be assessed as to whether they identify data subjects or groups. This makes measuring and assessing algorithmic privacy partially dependent upon the explainability and interpretability of the AI system. \"), mdx(\"p\", null, \"As part of a complex system, the data-algorithm interaction must also be assessed to assure against vulnerabilities arising from the relationship between the data and algorithm. \"), mdx(\"p\", null, \"System privacy is also an important subject matter for assuring the security of the system. For example, assessing the system for privacy vulnerabilities leaving it open to adversarial or malicious uses.\"), mdx(\"p\", null, \"It is important to note that in some cases, privacy and data protection diverge as subject matter for assurance. For example, model inferences or predictions may impact an individual\\u2019s or group\\u2019s privacy without involving personal data and engaging data protection. Similarly, data protection regulation contains articles on the right to an explanation for an algorithmic decision, which is not necessarily privacy related. \"))), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Robustness\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Model specification can be quantitatively assessed. Qualitative assessment required to establish acceptable benchmarks for robustness for a specific use case.\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Performance testing and formal verification required to test the specification of the model - testing the relationship that must hold between the inputs and outputs of the system across variations in input data.\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), mdx(\"p\", null, \"The robustness of an AI system corresponds to how effectively the system can be deemed safe and reliable across variations in the input variables used to make predictions.\"), mdx(\"p\", null, \"Evidence about robustness is required to ensure AI systems will be accurate and safe when faced with unexpected events that occur naturally during operation or if they are tampered with by a malicious actor.\"), mdx(\"p\", null, \"Different types of AI systems will have different robustness properties and different robustness bounds (acceptable levels of robustness). For example, a computer vision system for an autonomous vehicle will require a high level of robustness due to the serious potential harms of malfunction. Robustness properties for a computer vision system in an autonomous vehicle include: Image noise, deviations from optimal image conditions e.g. a branch partially covering a sign; geometric transformations e.g. road signs shifted at different angles; colour transformations e.g. differences in performance in daylight and nighttime.\"), mdx(\"p\", null, \"Performance testing metrics and formal verification can be used to test and quantify a system\\u2019s robustness properties. However, judgement is required to decide on the relevant robustness properties and what robustness scores are acceptable. \"))), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Explainability / interpretability\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Explainability / interpretability dependent on characteristics of an AI system\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Technical AI explainability toolkits, ICO/Turing Explaining Decision made with AI Guidance\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), mdx(\"p\", null, \"Explainability refers to the extent to which an AI system provides decisions or suggestions that can be understood by their users and developers. Interpretability refers to how easily cause and effect can be determined for the outputs of a system. \"), mdx(\"p\", null, \"Acceptable levels of explainability and interpretability vary depending on whether the use case is high or low risk. In high risk cases, drawing unjustified conclusions from data could cause serious harm.Explainability can be either an inherent characteristic of an AI system or it can be approximated by other methods. This latter type of explainability can be important for so-called \\u2018black box\\u2019 models like artificial neural networks. \"), mdx(\"p\", null, \"Explainability and interpretability are not always ends in themselves for assurance but enable systems to be audited and understood to come to conclusions about their trustworthiness. However, explainability and interpretability of a model are important in their own right for developers to check their models beyond just their performance, to rule out that a system is making predictions based upon meta-data rather than the specified input data. A famous example is of an algorithm that distinguished between wolves and huskies, yet this was driven entirely by whether there was a snowy background or not.\"), mdx(\"p\", null, \"A further dimension of explainability concerns the right to an explanation. For example, the transparency principle laid out in articles 13-25 in the GDPR requires AI developers and controllers to provide \\u2018meaningful information about the logic involved\\u2019 in a model or algorithm. This problem relates to data protection, but not privacy.\"), mdx(\"p\", null, \"These articles set out a right to an explanation. The ICO/Turing Guidance on Explaining Decisions made with AI sets out requirements for AI developers or controllers for providing meaningful information to explain a decision:\", mdx(\"ul\", null, mdx(\"li\", null, \"The type of information collected or used in creating the profile or making the automated decision;\"), mdx(\"li\", null, \"why this information is relevant; and\"), mdx(\"li\", null, \"what the likely impact is going to be/how it's likely to affect data subjects\"))), mdx(\"p\", null, \"For example, if a credit scoring organisation uses an automated process to assess a customer\\u2019s creditworthiness. For a traditional credit score, they will need to process data relating to the customer\\u2019s credit history, such as payment history, length of credit history, number of credit accounts. An alternative credit scoring agency may additionally use personality data or social media data.\"), mdx(\"p\", null, \"The credit scoring agency will need to explain what type of data is collected to build the credit profile, why this data is relevant to prediction the customers credit score, and the likely impact that such a score will have on the customer I.e. being approved for or denied credit.\"))), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Intended use / benefit\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"The scope and acceptable use cases for an AI system need to be qualitatively assessed based on the function of the AI system and the goals for using it. This assessment relies on evidence from quantitative assessment of accuracy, robustness, bias to determine use case suitability.\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Performance testing required for assessing robustness and bias audit required for assessing fairness of a system. Impact assessment required for understanding the potential impacts of using an AI system in a specific use case.\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), mdx(\"p\", null, \"To build trust in whether an AI system is suitable for the context or task for which it is being used requires evaluating evidence about it\\u2019s performance and robustness and considering the potential societal impacts or safety issues the system might cause if it were to be deployed.\"), mdx(\"p\", null, \"For example, the robustness, accuracy, fairness and explainability of an AI system will need to be very high if its intended use is in a medical imaging device. Because of the specific, complex and high-risk nature of medical imaging, an AI system could be certified for a very narrow set of uses or even a single task.\"), mdx(\"p\", null, \"In lower risk applications an AI system may be certified for use in a number of different contexts and across a number of different tasks.\"))), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Management systems and internal controls\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Management systems and internal controls need to be assessed qualitatively but can be assessed objectively against explicit standards and criteria e.g. ISO 9001 Quality Management standards.\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Compliance audits, including systems and process audits required to assess compliance of management processes including internal controls and quality management systems with standards, guidelines and regulations.\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), mdx(\"p\", null, \"Management systems and internal controls cover both technical and non technical aspects of AI system development, deployment and use. Management systems are used to assure complex socio-technical processes including quality and risk management.\"), mdx(\"p\", null, \"Management system standards provide explicit requirements against which organisational management can be assessed and its trustworthiness understood by assurance users. \"))), mdx(\"tr\", {\n    parentName: \"tbody\"\n  }, mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"Security\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"AI security can be assured at three levels: 1) Assessment of the product, system or service 2) Assessment of the development process of the product, system or service 3) assessment of the environment, such as the security management and governance.\"), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), mdx(\"p\", null, \"Security compliance audits, threat modelling and identification, Red teaming (penetration testing), product/service performance evaluation\"), mdx(\"p\", null, \"Available examples of technical standards:\", mdx(\"ul\", null, mdx(\"li\", null, \"Applicable to products: ISO/IEC 15408 Information technology\\u2014Security techniques\\u2014Evaluation criteria for IT security\"), mdx(\"li\", null, \"Applicable to processes: ISO/IEC 21827 :2002 Information technology\\u2014 Systems Security Engineering\\u2014Capability Maturity Model\"), mdx(\"li\", null, \"Applicable to security management: ISO/IEC 27001 Information technology\\u2014Security techniques\\u2014Information security management systems\\u2014 Requirements\")))), mdx(\"td\", _extends({\n    parentName: \"tr\"\n  }, {\n    \"align\": \"left\"\n  }), \"ISO/IEC TR 15443 defines these three high-level approaches as follows:\", mdx(\"ol\", null, mdx(\"li\", null, \"For the assessment of a product, system, service - In this case, assurance methods examine the product, system or service and its associated security design documentation independent of the development processes.\"), mdx(\"li\", null, \"Assessment of a process involves examining the organisational processes used in the production and operation of the product, system or service throughout its life cycle (i.e., development, deployment, delivery, testing, maintenance, disposal). Assurance is gained through the inference that the processes implemented by people affect the quality of the development and implementation of the product, system or service and, therefore, yield security assurance.\"), mdx(\"li\", null, \"Assessment of the environment involves an examination of the environmental factors that contribute to the quality of the processes and the production of the product, system or service. This type of assurance does not examine a deliverable or process directly. These factors assessed include personnel and physical facilities (e.g., development, production, delivery, operation).\")))))));\n}\n;\nMDXContent.isMDXComponent = true;","headings":[{"depth":2,"value":"Table: Mapping example AI assurance subject matter to suitable assurance techniques"}]}},"pageContext":{"slug":"/applying-assurance-techniques/","prev":{"label":"Assurance across the AI system lifecycle","link":"/lifecycle"},"next":{"label":"The role of independence in assuring AI","link":"/independence"}}}}